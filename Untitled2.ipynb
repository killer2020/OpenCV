{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#from utils import *\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 1438 ,Vocab size 2805 \n"
     ]
    }
   ],
   "source": [
    "#data = open('data.txt', 'r').read()\n",
    "#print(data)\n",
    "words=[]\n",
    "data=[]\n",
    "with open('data.txt','r') as f:\n",
    "    for line in f:\n",
    "        line = line.replace(\"\\n\", \" zzzzz\").strip() \n",
    "        data.append(line)\n",
    "        for word in line.split():\n",
    "           words.append(word)   \n",
    "\n",
    "words = list(set(words))\n",
    "\n",
    "print('Data size %d ,Vocab size %d ' % (len(data), len(words)))\n",
    "#print(words)\n",
    "#print(data)\n",
    "\n",
    "\n",
    "#data_size, vocab_size = len(data), len(chars)\n",
    "#print('There are %d total characters and %d unique characters in your data.' % (data_size, vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'a', 1: 'aaliyah', 2: 'abandoned', 3: 'abba', 4: 'able', 5: 'aborted', 6: 'about', 7: 'above', 8: 'absa', 9: 'academy', 10: 'access', 11: 'accolades', 12: 'accused', 13: 'accuses', 14: 'acquisition', 15: 'act', 16: 'action', 17: 'actor', 18: 'actors', 19: 'actress', 20: 'acts', 21: 'ad', 22: 'addicks', 23: 'addicts', 24: 'address', 25: 'adds', 26: 'adelaide', 27: 'admission', 28: 'admits', 29: 'adoption', 30: 'ads', 31: 'adventure', 32: 'advice', 33: 'aer', 34: 'afghanistan', 35: 'africa', 36: 'african', 37: 'after', 38: 'again', 39: 'against', 40: 'agassi', 41: 'age', 42: 'agency', 43: 'agenda', 44: 'agent', 45: 'agree', 46: 'agrees', 47: 'aguilera', 48: 'ahead', 49: 'ahold', 50: 'aid', 51: 'aides', 52: 'aids', 53: 'ailing', 54: 'aim', 55: 'aiming', 56: 'aims', 57: 'air', 58: 'aires', 59: 'airline', 60: 'airlines', 61: 'airport', 62: 'airways', 63: 'ajax', 64: 'album', 65: 'alert', 66: 'alexander', 67: 'algeria', 68: 'alicia', 69: 'alive', 70: 'allowed', 71: 'ally', 72: 'almagro', 73: 'almunia', 74: 'alternatives', 75: 'ambani', 76: 'america', 77: 'amid', 78: 'amnesty', 79: 'an', 80: 'analyst', 81: 'and', 82: 'anelka', 83: 'anfield', 84: 'anger', 85: 'angered', 86: 'angolan', 87: 'angry', 88: 'anil', 89: 'animation', 90: 'announce', 91: 'announced', 92: 'announces', 93: 'another', 94: 'answer', 95: 'answers', 96: 'apologise', 97: 'apologises', 98: 'apology', 99: 'appeal', 100: 'appeals', 101: 'appear', 102: 'applauds', 103: 'apple', 104: 'appoint', 105: 'appoints', 106: 'approach', 107: 'approvals', 108: 'approves', 109: 'aragones', 110: 'archer', 111: 'are', 112: 'argentina', 113: 'argentine', 114: 'argonaut', 115: 'arm', 116: 'arms', 117: 'army', 118: 'arnesen', 119: 'arnold', 120: 'around', 121: 'arrest', 122: 'arrested', 123: 'arrests', 124: 'arsenal', 125: 'art', 126: 'arts', 127: 'as', 128: 'asbestos', 129: 'ashley', 130: 'asia', 131: 'asian', 132: 'ask', 133: 'asks', 134: 'assess', 135: 'asset', 136: 'assets', 137: 'aston', 138: 'astrazeneca', 139: 'asylum', 140: 'at', 141: 'athens', 142: 'athletics', 143: 'atms', 144: 'attack', 145: 'attacked', 146: 'attacking', 147: 'attacks', 148: 'attend', 149: 'auckland', 150: 'auctions', 151: 'audio', 152: 'aurora', 153: 'auschwitz', 154: 'aussie', 155: 'australia', 156: 'australian', 157: 'austria', 158: 'author', 159: 'avert', 160: 'aviation', 161: 'aviator', 162: 'awaits', 163: 'award', 164: 'awarded', 165: 'awards', 166: 'away', 167: 'axa', 168: 'axe', 169: 'axed', 170: 'ba', 171: 'baa', 172: 'babayaro', 173: 'baby', 174: 'bach', 175: 'back', 176: 'backing', 177: 'backs', 178: 'bad', 179: 'bafta', 180: 'baghdad', 181: 'bail', 182: 'balco', 183: 'ballet', 184: 'ballooned', 185: 'ballot', 186: 'ban', 187: 'band', 188: 'bangkok', 189: 'bank', 190: 'banker', 191: 'banking', 192: 'bankruptcy', 193: 'banks', 194: 'banned', 195: 'bans', 196: 'barca', 197: 'barcelona', 198: 'barclays', 199: 'bargain', 200: 'barkley', 201: 'baron', 202: 'baros', 203: 'barred', 204: 'bash', 205: 'bassist', 206: 'bat', 207: 'bates', 208: 'bath', 209: 'battered', 210: 'battle', 211: 'battlefront', 212: 'battles', 213: 'bbc', 214: 'be', 215: 'beastie', 216: 'beat', 217: 'beats', 218: 'beattie', 219: 'became', 220: 'beckham', 221: 'become', 222: 'becomes', 223: 'beer', 224: 'bees', 225: 'begins', 226: 'behind', 227: 'beijingers', 228: 'beirut', 229: 'bekele', 230: 'bell', 231: 'bellamy', 232: 'benefit', 233: 'benin', 234: 'bening', 235: 'benitez', 236: 'berates', 237: 'bergamasco', 238: 'berlin', 239: 'bernabeu', 240: 'beset', 241: 'best', 242: 'better', 243: 'betting', 244: 'bid', 245: 'bidder', 246: 'bidding', 247: 'bids', 248: 'big', 249: 'bigger', 250: 'biggest', 251: 'bigots', 252: 'bill', 253: 'billboard', 254: 'billions', 255: 'biometric', 256: 'birmingham', 257: 'birthday', 258: 'black', 259: 'blackadder', 260: 'blackberry', 261: 'blackburn', 262: 'blair', 263: 'blame', 264: 'blames', 265: 'blasts', 266: 'blessing', 267: 'blighted', 268: 'blinx', 269: 'blitz', 270: 'block', 271: 'blocked', 272: 'blog', 273: 'blogger', 274: 'blogging', 275: 'blogs', 276: 'blow', 277: 'blows', 278: 'blues', 279: 'blunkett', 280: 'bmw', 281: 'bnp', 282: 'boasts', 283: 'boateng', 284: 'body', 285: 'boeing', 286: 'boerse', 287: 'bollywood', 288: 'bolton', 289: 'bomb', 290: 'bombardier', 291: 'bombs', 292: 'bond', 293: 'bonus', 294: 'bonuses', 295: 'boogeyman', 296: 'book', 297: 'booker', 298: 'bookmakers', 299: 'books', 300: 'boom', 301: 'booming', 302: 'boost', 303: 'boosted', 304: 'boosts', 305: 'boothroyd', 306: 'borders', 307: 'boris', 308: 'boro', 309: 'bortolami', 310: 'borussia', 311: 'boss', 312: 'bosses', 313: 'bosvelt', 314: 'bought', 315: 'bowl', 316: 'bows', 317: 'box', 318: 'boycott', 319: 'boys', 320: 'bp', 321: 'brands', 322: 'branson', 323: 'brawl', 324: 'brazil', 325: 'brazilian', 326: 'breakneck', 327: 'breaks', 328: 'breakthrough', 329: 'brennan', 330: 'brentford', 331: 'bribery', 332: 'bridge', 333: 'bring', 334: 'brit', 335: 'britain', 336: 'british', 337: 'briton', 338: 'britons', 339: 'brits', 340: 'broadband', 341: 'broadway', 342: 'broken', 343: 'brookside', 344: 'brother', 345: 'brown', 346: 'browse', 347: 'browser', 348: 'brussels', 349: 'bryan', 350: 'bt', 351: 'budget', 352: 'buenos', 353: 'buffets', 354: 'buffy', 355: 'bug', 356: 'bugs', 357: 'build', 358: 'building', 359: 'bulgaria', 360: 'bullish', 361: 'burglar', 362: 'burnley', 363: 'burren', 364: 'bus', 365: 'bush', 366: 'business', 367: 'businesses', 368: 'bust', 369: 'butler', 370: 'buy', 371: 'buying', 372: 'buys', 373: 'by', 374: 'cabinet', 375: 'cabs', 376: 'cactus', 377: 'cadbury', 378: 'cadillacs', 379: 'cairn', 380: 'calder', 381: 'caledonian', 382: 'california', 383: 'call', 384: 'called', 385: 'calls', 386: 'calms', 387: 'cameroon', 388: 'campaign', 389: 'campbell', 390: 'campese', 391: 'can', 392: 'canberra', 393: 'candela', 394: 'candidate', 395: 'cannabis', 396: 'cantona', 397: 'capra', 398: 'capriati', 399: 'captaincy', 400: 'captains', 401: 'car', 402: 'card', 403: 'cardinal', 404: 'cards', 405: 'care', 406: 'career', 407: 'careful', 408: 'carling', 409: 'carpet', 410: 'carroll', 411: 'carry', 412: 'cars', 413: 'carson', 414: 'cartridges', 415: 'case', 416: 'cash', 417: 'casino', 418: 'castaignede', 419: 'casts', 420: 'catch', 421: 'cautious', 422: 'cebit', 423: 'celebrates', 424: 'celebrities', 425: 'cell', 426: 'celtic', 427: 'celts', 428: 'cement', 429: 'ceremony', 430: 'cesar', 431: 'chain', 432: 'challenge', 433: 'challenges', 434: 'chambers', 435: 'champions', 436: 'chancellor', 437: 'chances', 438: 'change', 439: 'changes', 440: 'channel', 441: 'chaos', 442: 'charge', 443: 'charges', 444: 'charity', 445: 'charles', 446: 'charm', 447: 'charms', 448: 'chart', 449: 'charts', 450: 'charvis', 451: 'cheaper', 452: 'check', 453: 'checks', 454: 'cheers', 455: 'cheese', 456: 'chelsea', 457: 'chepkemei', 458: 'cherie', 459: 'chest', 460: 'chester', 461: 'chests', 462: 'chief', 463: 'chiefs', 464: 'child', 465: 'children', 466: 'chilean', 467: 'china', 468: 'chinese', 469: 'chip', 470: 'chris', 471: 'christina', 472: 'christmas', 473: 'church', 474: 'cinema', 475: 'circuit', 476: 'citigroup', 477: 'citizenship', 478: 'city', 479: 'civil', 480: 'claim', 481: 'claims', 482: 'clampdown', 483: 'clark', 484: 'clarke', 485: 'clash', 486: 'clashes', 487: 'classroom', 488: 'classy', 489: 'clean', 490: 'clear', 491: 'cleared', 492: 'clijsters', 493: 'climate', 494: 'climb', 495: 'climbdown', 496: 'clinches', 497: 'clock', 498: 'close', 499: 'closer', 500: 'closure', 501: 'club', 502: 'clubs', 503: 'clumsy', 504: 'coach', 505: 'coal', 506: 'coast', 507: 'cocoa', 508: 'code', 509: 'cole', 510: 'collect', 511: 'collins', 512: 'colludes', 513: 'colombia', 514: 'colour', 515: 'combat', 516: 'come', 517: 'comeback', 518: 'comedy', 519: 'comes', 520: 'comic', 521: 'coming', 522: 'commit', 523: 'commodore', 524: 'commons', 525: 'companies', 526: 'company', 527: 'compete', 528: 'competition', 529: 'complaint', 530: 'completes', 531: 'composer', 532: 'computer', 533: 'concern', 534: 'concerns', 535: 'concert', 536: 'concerts', 537: 'conference', 538: 'confidence', 539: 'confident', 540: 'confirms', 541: 'congratulated', 542: 'congratulates', 543: 'connick', 544: 'connors', 545: 'consensus', 546: 'conservative', 547: 'considering', 548: 'considers', 549: 'consistency', 550: 'console', 551: 'consoles', 552: 'constituency', 553: 'construction', 554: 'consultant', 555: 'consumer', 556: 'consumers', 557: 'conte', 558: 'contest', 559: 'continue', 560: 'continues', 561: 'contract', 562: 'contracts', 563: 'control', 564: 'controversial', 565: 'controversy', 566: 'conviction', 567: 'cool', 568: 'copies', 569: 'copper', 570: 'copy', 571: 'copyright', 572: 'coria', 573: 'corp', 574: 'correction', 575: 'corry', 576: 'cost', 577: 'costs', 578: 'could', 579: 'councils', 580: 'country', 581: 'coup', 582: 'course', 583: 'court', 584: 'courtney', 585: 'courts', 586: 'crash', 587: 'create', 588: 'creator', 589: 'credit', 590: 'criminal', 591: 'criminally', 592: 'criminals', 593: 'crisis', 594: 'critical', 595: 'criticise', 596: 'criticised', 597: 'criticises', 598: 'criticism', 599: 'criticisms', 600: 'critics', 601: 'cross', 602: 'crowd', 603: 'crowds', 604: 'crown', 605: 'crucial', 606: 'crunch', 607: 'csi', 608: 'cuba', 609: 'cudicini', 610: 'cult', 611: 'cunningham', 612: 'cup', 613: 'curbishley', 614: 'currency', 615: 'cut', 616: 'cutbacks', 617: 'cuts', 618: 'cutting', 619: 'cyber', 620: 'cyril', 621: 'da', 622: 'dal', 623: 'dallaglio', 624: 'damage', 625: 'dame', 626: 'dance', 627: 'darfur', 628: 'dark', 629: 'data', 630: 'date', 631: 'dating', 632: 'davenport', 633: 'david', 634: 'davies', 635: 'davis', 636: 'davos', 637: 'dawson', 638: 'day', 639: 'days', 640: 'de', 641: 'dead', 642: 'deadline', 643: 'deaf', 644: 'deal', 645: 'deals', 646: 'death', 647: 'debate', 648: 'debra', 649: 'debt', 650: 'debut', 651: 'debuts', 652: 'decade', 653: 'december', 654: 'decides', 655: 'decision', 656: 'decisions', 657: 'decline', 658: 'deep', 659: 'deeper', 660: 'defeat', 661: 'defeatism', 662: 'defection', 663: 'defectors', 664: 'defects', 665: 'defence', 666: 'defends', 667: 'defiant', 668: 'deficit', 669: 'deflects', 670: 'dejected', 671: 'delay', 672: 'delayed', 673: 'delays', 674: 'delight', 675: 'delighted', 676: 'delta', 677: 'deluge', 678: 'demand', 679: 'demands', 680: 'dementieva', 681: 'demo', 682: 'demons', 683: 'dems', 684: 'denied', 685: 'denies', 686: 'denise', 687: 'dent', 688: 'deport', 689: 'desailly', 690: 'describes', 691: 'design', 692: 'desktop', 693: 'desperate', 694: 'despite', 695: 'details', 696: 'detain', 697: 'detainees', 698: 'detentions', 699: 'determined', 700: 'deutsche', 701: 'development', 702: 'devon', 703: 'diageo', 704: 'dialler', 705: 'dicaprio', 706: 'dies', 707: 'diesel', 708: 'diet', 709: 'difficult', 710: 'dig', 711: 'digital', 712: 'digs', 713: 'dimension', 714: 'dip', 715: 'diploma', 716: 'dips', 717: 'direct', 718: 'director', 719: 'directors', 720: 'disabled', 721: 'disallowed', 722: 'disappointed', 723: 'disaster', 724: 'discipline', 725: 'disclosure', 726: 'discuss', 727: 'dismantles', 728: 'dismissed', 729: 'dismisses', 730: 'disney', 731: 'display', 732: 'dispute', 733: 'disputed', 734: 'dividend', 735: 'divides', 736: 'division', 737: 'diy', 738: 'dj', 739: 'djs', 740: 'dog', 741: 'dogged', 742: 'doherty', 743: 'dollar', 744: 'domain', 745: 'domains', 746: 'dominate', 747: 'dominates', 748: 'dominici', 749: 'donations', 750: 'door', 751: 'doors', 752: 'doping', 753: 'dortmund', 754: 'double', 755: 'doubled', 756: 'doubt', 757: 'doubters', 758: 'doubts', 759: 'douglas', 760: 'dour', 761: 'doves', 762: 'down', 763: 'downing', 764: 'download', 765: 'downloads', 766: 'dozens', 767: 'drags', 768: 'drake', 769: 'drama', 770: 'draw', 771: 'draws', 772: 'drinking', 773: 'drive', 774: 'driven', 775: 'drives', 776: 'drop', 777: 'dropped', 778: 'drops', 779: 'drug', 780: 'drugs', 781: 'ds', 782: 'dual', 783: 'dubai', 784: 'dudek', 785: 'due', 786: 'duff', 787: 'dunne', 788: 'duo', 789: 'duran', 790: 'durex', 791: 'dutch', 792: 'dvd', 793: 'ea', 794: 'eads', 795: 'early', 796: 'earn', 797: 'earnings', 798: 'earns', 799: 'ease', 800: 'eases', 801: 'easy', 802: 'ebbers', 803: 'ec', 804: 'ecb', 805: 'eco', 806: 'economic', 807: 'economy', 808: 'edge', 809: 'edges', 810: 'edgy', 811: 'edinburgh', 812: 'edu', 813: 'edwards', 814: 'effect', 815: 'egypt', 816: 'egyptian', 817: 'eight', 818: 'el', 819: 'elated', 820: 'elderly', 821: 'election', 822: 'electrolux', 823: 'electronics', 824: 'elia', 825: 'elliot', 826: 'elton', 827: 'elvis', 828: 'embargo', 829: 'embassies', 830: 'emi', 831: 'eminem', 832: 'emotional', 833: 'empire', 834: 'employ', 835: 'encounter', 836: 'end', 837: 'ended', 838: 'ending', 839: 'ends', 840: 'energy', 841: 'engines', 842: 'england', 843: 'english', 844: 'enjoys', 845: 'enter', 846: 'enters', 847: 'epic', 848: 'episode', 849: 'equal', 850: 'ericsson', 851: 'error', 852: 'erupts', 853: 'escape', 854: 'escaped', 855: 'ethnic', 856: 'eu', 857: 'euro', 858: 'eurodisney', 859: 'euronext', 860: 'europe', 861: 'european', 862: 'evans', 863: 'everton', 864: 'evicted', 865: 'eviction', 866: 'evidence', 867: 'excludes', 868: 'executive', 869: 'executives', 870: 'exel', 871: 'exeter', 872: 'exit', 873: 'exits', 874: 'exorcist', 875: 'expands', 876: 'expected', 877: 'expects', 878: 'expert', 879: 'explodes', 880: 'export', 881: 'exports', 882: 'expression', 883: 'extend', 884: 'extra', 885: 'eye', 886: 'eyeing', 887: 'eyes', 888: 'fa', 889: 'face', 890: 'faced', 891: 'faces', 892: 'facing', 893: 'factor', 894: 'factories', 895: 'factory', 896: 'fail', 897: 'fails', 898: 'failure', 899: 'fair', 900: 'faith', 901: 'fake', 902: 'faking', 903: 'fall', 904: 'falls', 905: 'faltered', 906: 'fame', 907: 'famed', 908: 'families', 909: 'family', 910: 'famous', 911: 'fannie', 912: 'fans', 913: 'fantasy', 914: 'fao', 915: 'fares', 916: 'farina', 917: 'farrell', 918: 'fast', 919: 'fatboy', 920: 'faultless', 921: 'faulty', 922: 'favourite', 923: 'favourites', 924: 'favours', 925: 'fbi', 926: 'fc', 927: 'fear', 928: 'fears', 929: 'february', 930: 'fed', 931: 'federer', 932: 'fees', 933: 'female', 934: 'ferdinand', 935: 'ferguson', 936: 'ferrari', 937: 'ferrero', 938: 'fervour', 939: 'festival', 940: 'festive', 941: 'feta', 942: 'feted', 943: 'feud', 944: 'fever', 945: 'few', 946: 'fiat', 947: 'fifa', 948: 'fight', 949: 'fights', 950: 'fightstar', 951: 'figures', 952: 'files', 953: 'film', 954: 'films', 955: 'final', 956: 'finance', 957: 'financial', 958: 'find', 959: 'finding', 960: 'finds', 961: 'fine', 962: 'fined', 963: 'fines', 964: 'fingerprints', 965: 'finish', 966: 'finnan', 967: 'fire', 968: 'fired', 969: 'firefox', 970: 'fires', 971: 'firm', 972: 'firms', 973: 'first', 974: 'fishing', 975: 'fit', 976: 'fitness', 977: 'five', 978: 'fix', 979: 'fletcher', 980: 'flexible', 981: 'flies', 982: 'flock', 983: 'flops', 984: 'flourished', 985: 'fly', 986: 'fockers', 987: 'focus', 988: 'focused', 989: 'focuses', 990: 'fold', 991: 'follow', 992: 'foot', 993: 'football', 994: 'for', 995: 'force', 996: 'forced', 997: 'ford', 998: 'forecasts', 999: 'forehead', 1000: 'foreign', 1001: 'form', 1002: 'former', 1003: 'forward', 1004: 'foster', 1005: 'fosters', 1006: 'found', 1007: 'founder', 1008: 'four', 1009: 'fowler', 1010: 'fox', 1011: 'foxx', 1012: 'fracas', 1013: 'france', 1014: 'franz', 1015: 'fraud', 1016: 'fraudster', 1017: 'free', 1018: 'french', 1019: 'fresh', 1020: 'friend', 1021: 'from', 1022: 'frontman', 1023: 'frustrated', 1024: 'fuel', 1025: 'fuels', 1026: 'fuller', 1027: 'fume', 1028: 'fuming', 1029: 'fund', 1030: 'funding', 1031: 'furlong', 1032: 'further', 1033: 'future', 1034: 'gadget', 1035: 'gadgets', 1036: 'gain', 1037: 'gains', 1038: 'galloway', 1039: 'galore', 1040: 'game', 1041: 'gamers', 1042: 'games', 1043: 'gaming', 1044: 'gangsters', 1045: 'gap', 1046: 'gaps', 1047: 'gardener', 1048: 'gas', 1049: 'gates', 1050: 'gather', 1051: 'gathers', 1052: 'gatlin', 1053: 'gaudio', 1054: 'gb', 1055: 'gdp', 1056: 'gear', 1057: 'gebrselassie', 1058: 'genius', 1059: 'george', 1060: 'georgia', 1061: 'german', 1062: 'germans', 1063: 'germany', 1064: 'gerrard', 1065: 'gervais', 1066: 'get', 1067: 'gets', 1068: 'ghosn', 1069: 'giant', 1070: 'giants', 1071: 'gift', 1072: 'gifts', 1073: 'gig', 1074: 'gigapixel', 1075: 'giggs', 1076: 'gilbert', 1077: 'ginepri', 1078: 'give', 1079: 'given', 1080: 'gives', 1081: 'giving', 1082: 'gizmondo', 1083: 'glasgow', 1084: 'glastonbury', 1085: 'glaxo', 1086: 'glazer', 1087: 'global', 1088: 'globe', 1089: 'globes', 1090: 'gloom', 1091: 'glory', 1092: 'gloucester', 1093: 'gm', 1094: 'go', 1095: 'goals', 1096: 'god', 1097: 'godzilla', 1098: 'goes', 1099: 'gogh', 1100: 'gold', 1101: 'golden', 1102: 'goldsmith', 1103: 'golf', 1104: 'gongs', 1105: 'good', 1106: 'goodrem', 1107: 'google', 1108: 'government', 1109: 'grab', 1110: 'graft', 1111: 'grammys', 1112: 'grand', 1113: 'grass', 1114: 'gravesen', 1115: 'greater', 1116: 'greek', 1117: 'green', 1118: 'greener', 1119: 'greeted', 1120: 'grewcock', 1121: 'grid', 1122: 'griffin', 1123: 'grinds', 1124: 'grips', 1125: 'gritty', 1126: 'gronkjaer', 1127: 'grounded', 1128: 'group', 1129: 'grow', 1130: 'growing', 1131: 'grows', 1132: 'growth', 1133: 'gsk', 1134: 'gta', 1135: 'guantanamo', 1136: 'guardian', 1137: 'guerrouj', 1138: 'guidelines', 1139: 'guild', 1140: 'guilty', 1141: 'guns', 1142: 'gurkhas', 1143: 'hail', 1144: 'hails', 1145: 'half', 1146: 'halifax', 1147: 'halloween', 1148: 'halt', 1149: 'halts', 1150: 'hamm', 1151: 'hamstring', 1152: 'hand', 1153: 'handed', 1154: 'handheld', 1155: 'handhelds', 1156: 'hands', 1157: 'hanks', 1158: 'hanover', 1159: 'hantuchova', 1160: 'happy', 1161: 'hard', 1162: 'harinordoquy', 1163: 'hariri', 1164: 'harry', 1165: 'has', 1166: 'hatfield', 1167: 'hayes', 1168: 'he', 1169: 'head', 1170: 'heading', 1171: 'heads', 1172: 'healey', 1173: 'health', 1174: 'hear', 1175: 'hearing', 1176: 'heath', 1177: 'held', 1178: 'help', 1179: 'helps', 1180: 'hendrix', 1181: 'henman', 1182: 'henry', 1183: 'henson', 1184: 'her', 1185: 'heralds', 1186: 'heroics', 1187: 'hewitt', 1188: 'hidden', 1189: 'high', 1190: 'highbury', 1191: 'highlight', 1192: 'highs', 1193: 'hill', 1194: 'hillbillies', 1195: 'hingis', 1196: 'hints', 1197: 'his', 1198: 'historic', 1199: 'history', 1200: 'hit', 1201: 'hitch', 1202: 'hits', 1203: 'hiv', 1204: 'hoddle', 1205: 'hodges', 1206: 'hodgson', 1207: 'hoffman', 1208: 'hold', 1209: 'holders', 1210: 'holds', 1211: 'holiff', 1212: 'hollywood', 1213: 'holmes', 1214: 'home', 1215: 'homebuyers', 1216: 'homes', 1217: 'honda', 1218: 'hong', 1219: 'honour', 1220: 'honours', 1221: 'hope', 1222: 'hopes', 1223: 'hopman', 1224: 'horror', 1225: 'host', 1226: 'hostage', 1227: 'hosts', 1228: 'hotel', 1229: 'hots', 1230: 'hotspot', 1231: 'houllier', 1232: 'hour', 1233: 'house', 1234: 'housing', 1235: 'hovers', 1236: 'how', 1237: 'howard', 1238: 'hp', 1239: 'huge', 1240: 'humanoid', 1241: 'hundreds', 1242: 'hunt', 1243: 'hunting', 1244: 'hunts', 1245: 'hybrid', 1246: 'hyundai', 1247: 'iaaf', 1248: 'ibm', 1249: 'id', 1250: 'idea', 1251: 'idol', 1252: 'idowu', 1253: 'illness', 1254: 'imf', 1255: 'immigration', 1256: 'impact', 1257: 'impersonator', 1258: 'impressed', 1259: 'impressive', 1260: 'improve', 1261: 'improves', 1262: 'in', 1263: 'income', 1264: 'increase', 1265: 'increases', 1266: 'incredibles', 1267: 'india', 1268: 'indian', 1269: 'indicates', 1270: 'indie', 1271: 'indies', 1272: 'indonesians', 1273: 'indoor', 1274: 'industrial', 1275: 'industry', 1276: 'indy', 1277: 'inflation', 1278: 'influence', 1279: 'injection', 1280: 'injury', 1281: 'ink', 1282: 'innovation', 1283: 'inquiry', 1284: 'inside', 1285: 'insurance', 1286: 'insurer', 1287: 'insurers', 1288: 'intel', 1289: 'interactive', 1290: 'interest', 1291: 'internet', 1292: 'into', 1293: 'invention', 1294: 'investment', 1295: 'investor', 1296: 'invite', 1297: 'ipod', 1298: 'iran', 1299: 'iranian', 1300: 'iraq', 1301: 'iraqi', 1302: 'iraqis', 1303: 'ireland', 1304: 'irish', 1305: 'is', 1306: 'isinbayeva', 1307: 'islamic', 1308: 'isle', 1309: 'israel', 1310: 'israeli', 1311: 'issue', 1312: 'issued', 1313: 'issues', 1314: 'it', 1315: 'italy', 1316: 'its', 1317: 'itunes', 1318: 'ivanovic', 1319: 'ivory', 1320: 'jack', 1321: 'jail', 1322: 'jailed', 1323: 'jailings', 1324: 'jakarta', 1325: 'jamaica', 1326: 'james', 1327: 'jamieson', 1328: 'jansen', 1329: 'japan', 1330: 'japanese', 1331: 'jarvis', 1332: 'jeeves', 1333: 'jet', 1334: 'jewish', 1335: 'jibe', 1336: 'job', 1337: 'jobless', 1338: 'jobs', 1339: 'johansson', 1340: 'john', 1341: 'johnny', 1342: 'johnson', 1343: 'join', 1344: 'joins', 1345: 'jol', 1346: 'jones', 1347: 'jose', 1348: 'jowell', 1349: 'joy', 1350: 'jp', 1351: 'jr', 1352: 'judge', 1353: 'julie', 1354: 'jump', 1355: 'june', 1356: 'jungle', 1357: 'juninho', 1358: 'junk', 1359: 'juventus', 1360: 'karachi', 1361: 'kasabian', 1362: 'kashmir', 1363: 'kazakh', 1364: 'keane', 1365: 'keanu', 1366: 'keaveney', 1367: 'keegan', 1368: 'keen', 1369: 'keep', 1370: 'keeper', 1371: 'keeping', 1372: 'keeps', 1373: 'kelly', 1374: 'ken', 1375: 'kennedy', 1376: 'kenteris', 1377: 'kenya', 1378: 'kenyan', 1379: 'kenyon', 1380: 'kerr', 1381: 'kevin', 1382: 'kewell', 1383: 'key', 1384: 'keys', 1385: 'khodorkovsky', 1386: 'kidman', 1387: 'killing', 1388: 'kilroy', 1389: 'king', 1390: 'kinnock', 1391: 'kirwan', 1392: 'kit', 1393: 'klinsmann', 1394: 'kluft', 1395: 'knocks', 1396: 'kong', 1397: 'korea', 1398: 'korean', 1399: 'koubek', 1400: 'kraft', 1401: 'kreme', 1402: 'krispy', 1403: 'kuznetsova', 1404: 'la', 1405: 'label', 1406: 'labour', 1407: 'lack', 1408: 'lacklustre', 1409: 'lacroix', 1410: 'laments', 1411: 'land', 1412: 'landmark', 1413: 'laporte', 1414: 'laptop', 1415: 'laser', 1416: 'lasers', 1417: 'last', 1418: 'lasting', 1419: 'late', 1420: 'latest', 1421: 'latin', 1422: 'laud', 1423: 'launched', 1424: 'launches', 1425: 'laura', 1426: 'law', 1427: 'laws', 1428: 'lawsuit', 1429: 'lead', 1430: 'leader', 1431: 'leaders', 1432: 'leading', 1433: 'leads', 1434: 'leaflet', 1435: 'league', 1436: 'leaks', 1437: 'learning', 1438: 'learns', 1439: 'lease', 1440: 'leave', 1441: 'leaves', 1442: 'lee', 1443: 'leeds', 1444: 'legal', 1445: 'legend', 1446: 'legendary', 1447: 'lehmann', 1448: 'lender', 1449: 'lengthy', 1450: 'lennon', 1451: 'lesotho', 1452: 'lessons', 1453: 'lets', 1454: 'level', 1455: 'levy', 1456: 'lewsey', 1457: 'lib', 1458: 'liberian', 1459: 'libraries', 1460: 'library', 1461: 'licence', 1462: 'lid', 1463: 'life', 1464: 'lifestyle', 1465: 'lift', 1466: 'lifts', 1467: 'limit', 1468: 'line', 1469: 'lingus', 1470: 'lining', 1471: 'link', 1472: 'links', 1473: 'linux', 1474: 'lions', 1475: 'liquidation', 1476: 'lira', 1477: 'list', 1478: 'listing', 1479: 'lit', 1480: 'little', 1481: 'liverpool', 1482: 'llewellyn', 1483: 'loan', 1484: 'local', 1485: 'log', 1486: 'logs', 1487: 'lomu', 1488: 'london', 1489: 'long', 1490: 'look', 1491: 'looks', 1492: 'looms', 1493: 'lopez', 1494: 'lords', 1495: 'lose', 1496: 'loses', 1497: 'losing', 1498: 'loss', 1499: 'lost', 1500: 'love', 1501: 'low', 1502: 'lows', 1503: 'loyalty', 1504: 'lse', 1505: 'lufthansa', 1506: 'lunch', 1507: 'lure', 1508: 'lying', 1509: 'mac', 1510: 'madagascar', 1511: 'made', 1512: 'madrid', 1513: 'mae', 1514: 'mail', 1515: 'mainstream', 1516: 'major', 1517: 'make', 1518: 'maker', 1519: 'makes', 1520: 'making', 1521: 'malaysia', 1522: 'mallon', 1523: 'man', 1524: 'management', 1525: 'manager', 1526: 'manchester', 1527: 'mandarin', 1528: 'mandelson', 1529: 'mantle', 1530: 'manufacturing', 1531: 'many', 1532: 'mapping', 1533: 'marathon', 1534: 'march', 1535: 'marches', 1536: 'margin', 1537: 'mark', 1538: 'market', 1539: 'markets', 1540: 'mars', 1541: 'marseille', 1542: 'marsh', 1543: 'martinez', 1544: 'masks', 1545: 'maso', 1546: 'match', 1547: 'materials', 1548: 'mauresmo', 1549: 'mauritius', 1550: 'may', 1551: 'mayor', 1552: 'mccall', 1553: 'mcclaren', 1554: 'mcconnell', 1555: 'mccririck', 1556: 'mci', 1557: 'mcilroy', 1558: 'mcleish', 1559: 'medal', 1560: 'media', 1561: 'meet', 1562: 'meeting', 1563: 'melbourne', 1564: 'melzer', 1565: 'members', 1566: 'memories', 1567: 'mercedes', 1568: 'merge', 1569: 'merger', 1570: 'message', 1571: 'messages', 1572: 'metlife', 1573: 'mexicans', 1574: 'mexico', 1575: 'michael', 1576: 'michalak', 1577: 'michels', 1578: 'microsoft', 1579: 'mido', 1580: 'migrant', 1581: 'mikoliunas', 1582: 'milan', 1583: 'milburn', 1584: 'milk', 1585: 'million', 1586: 'millions', 1587: 'millwall', 1588: 'mini', 1589: 'minimum', 1590: 'minister', 1591: 'ministers', 1592: 'ministry', 1593: 'mirza', 1594: 'miss', 1595: 'misses', 1596: 'missing', 1597: 'mission', 1598: 'mitsubishi', 1599: 'mixed', 1600: 'mladenovic', 1601: 'mobile', 1602: 'mobiles', 1603: 'mock', 1604: 'models', 1605: 'modern', 1606: 'mogul', 1607: 'monfils', 1608: 'mongrel', 1609: 'monitor', 1610: 'moody', 1611: 'moore', 1612: 'more', 1613: 'moreno', 1614: 'morgan', 1615: 'morientes', 1616: 'morris', 1617: 'morrison', 1618: 'mosque', 1619: 'motive', 1620: 'mountain', 1621: 'mourinho', 1622: 'mouth', 1623: 'move', 1624: 'moves', 1625: 'movie', 1626: 'moving', 1627: 'moya', 1628: 'mp', 1629: 'mps', 1630: 'mrs', 1631: 'ms', 1632: 'msps', 1633: 'mtv', 1634: 'muggers', 1635: 'mull', 1636: 'mulls', 1637: 'multimedia', 1638: 'mumbai', 1639: 'munster', 1640: 'murder', 1641: 'murray', 1642: 'museum', 1643: 'music', 1644: 'musical', 1645: 'musicians', 1646: 'muslims', 1647: 'mutant', 1648: 'mutu', 1649: 'mystery', 1650: 'nadal', 1651: 'nalbandian', 1652: 'name', 1653: 'named', 1654: 'names', 1655: 'napster', 1656: 'narrow', 1657: 'narrows', 1658: 'nations', 1659: 'navratilova', 1660: 'nazi', 1661: 'ne', 1662: 'near', 1663: 'neeson', 1664: 'nelly', 1665: 'nestle', 1666: 'net', 1667: 'network', 1668: 'networks', 1669: 'never', 1670: 'new', 1671: 'newcastle', 1672: 'newest', 1673: 'newry', 1674: 'news', 1675: 'next', 1676: 'nhs', 1677: 'ni', 1678: 'nicely', 1679: 'nick', 1680: 'nigeria', 1681: 'nigerian', 1682: 'night', 1683: 'nike', 1684: 'nintendo', 1685: 'niro', 1686: 'nirvana', 1687: 'nissan', 1688: 'nistelrooy', 1689: 'no', 1690: 'nods', 1691: 'nominations', 1692: 'nominees', 1693: 'north', 1694: 'norwich', 1695: 'nostalgia', 1696: 'not', 1697: 'novartis', 1698: 'novel', 1699: 'november', 1700: 'now', 1701: 'nuclear', 1702: 'nudity', 1703: 'number', 1704: 'oaps', 1705: 'oasis', 1706: 'observers', 1707: 'october', 1708: 'odd', 1709: 'of', 1710: 'off', 1711: 'offer', 1712: 'offers', 1713: 'office', 1714: 'official', 1715: 'officials', 1716: 'offshore', 1717: 'oil', 1718: 'old', 1719: 'olympics', 1720: 'on', 1721: 'once', 1722: 'one', 1723: 'online', 1724: 'onslaught', 1725: 'open', 1726: 'opening', 1727: 'opens', 1728: 'opera', 1729: 'opinion', 1730: 'opposes', 1731: 'opposition', 1732: 'optimism', 1733: 'optimistic', 1734: 'option', 1735: 'opts', 1736: 'or', 1737: 'oracle', 1738: 'orange', 1739: 'order', 1740: 'ordered', 1741: 'ore', 1742: 'original', 1743: 'oscar', 1744: 'oscars', 1745: 'ossie', 1746: 'out', 1747: 'outkast', 1748: 'outline', 1749: 'outlines', 1750: 'outlining', 1751: 'output', 1752: 'outside', 1753: 'outspent', 1754: 'over', 1755: 'overcomes', 1756: 'overhaul', 1757: 'overseas', 1758: 'owen', 1759: 'own', 1760: 'owner', 1761: 'pace', 1762: 'packages', 1763: 'pain', 1764: 'pair', 1765: 'palace', 1766: 'palestinian', 1767: 'pandas', 1768: 'paper', 1769: 'paraguay', 1770: 'pardon', 1771: 'parents', 1772: 'paris', 1773: 'park', 1774: 'parker', 1775: 'parking', 1776: 'parmalat', 1777: 'parmar', 1778: 'parry', 1779: 'part', 1780: 'parties', 1781: 'party', 1782: 'passengers', 1783: 'passion', 1784: 'passport', 1785: 'past', 1786: 'patches', 1787: 'patent', 1788: 'patents', 1789: 'patrol', 1790: 'patsy', 1791: 'patti', 1792: 'paul', 1793: 'pavey', 1794: 'pay', 1795: 'payment', 1796: 'payments', 1797: 'payout', 1798: 'pc', 1799: 'pcs', 1800: 'peace', 1801: 'pearce', 1802: 'peel', 1803: 'peers', 1804: 'penalties', 1805: 'pension', 1806: 'pensions', 1807: 'people', 1808: 'perform', 1809: 'persia', 1810: 'persie', 1811: 'personal', 1812: 'perspective', 1813: 'pete', 1814: 'peugeot', 1815: 'philippoussis', 1816: 'phone', 1817: 'phones', 1818: 'photo', 1819: 'photographer', 1820: 'phytopharm', 1821: 'picked', 1822: 'picking', 1823: 'picks', 1824: 'picture', 1825: 'piero', 1826: 'pinochet', 1827: 'pioneers', 1828: 'pipeline', 1829: 'pirated', 1830: 'pirates', 1831: 'pitches', 1832: 'pixies', 1833: 'place', 1834: 'plan', 1835: 'planning', 1836: 'plans', 1837: 'plant', 1838: 'plasma', 1839: 'platform', 1840: 'play', 1841: 'players', 1842: 'playing', 1843: 'plays', 1844: 'playstation', 1845: 'plea', 1846: 'plead', 1847: 'pledge', 1848: 'pledges', 1849: 'plots', 1850: 'plunges', 1851: 'plus', 1852: 'pm', 1853: 'pocket', 1854: 'podcasts', 1855: 'pods', 1856: 'point', 1857: 'poised', 1858: 'police', 1859: 'policing', 1860: 'policy', 1861: 'political', 1862: 'politics', 1863: 'poll', 1864: 'pompey', 1865: 'poor', 1866: 'poppins', 1867: 'pops', 1868: 'porsche', 1869: 'portable', 1870: 'portfolio', 1871: 'portishead', 1872: 'portrait', 1873: 'position', 1874: 'poster', 1875: 'postponed', 1876: 'potential', 1877: 'potter', 1878: 'pountney', 1879: 'poverty', 1880: 'power', 1881: 'praises', 1882: 'predicts', 1883: 'premier', 1884: 'premiere', 1885: 'prepare', 1886: 'prepared', 1887: 'prepares', 1888: 'presidency', 1889: 'press', 1890: 'preston', 1891: 'prevails', 1892: 'price', 1893: 'prices', 1894: 'prince', 1895: 'printers', 1896: 'prisoner', 1897: 'privatisation', 1898: 'prize', 1899: 'probe', 1900: 'probes', 1901: 'problem', 1902: 'problems', 1903: 'processing', 1904: 'prodigy', 1905: 'producers', 1906: 'product', 1907: 'production', 1908: 'profit', 1909: 'profiteers', 1910: 'profits', 1911: 'program', 1912: 'progress', 1913: 'promise', 1914: 'promised', 1915: 'prompts', 1916: 'prop', 1917: 'property', 1918: 'proposes', 1919: 'pros', 1920: 'prospect', 1921: 'prosperity', 1922: 'protection', 1923: 'protest', 1924: 'protesters', 1925: 'proves', 1926: 'prutton', 1927: 'psp', 1928: 'pub', 1929: 'public', 1930: 'pull', 1931: 'pulling', 1932: 'pulls', 1933: 'pump', 1934: 'punish', 1935: 'punishment', 1936: 'purrs', 1937: 'push', 1938: 'pushed', 1939: 'put', 1940: 'putin', 1941: 'puts', 1942: 'puzzle', 1943: 'qantas', 1944: 'qpr', 1945: 'quake', 1946: 'quango', 1947: 'quarterly', 1948: 'quartet', 1949: 'queen', 1950: 'quell', 1951: 'queries', 1952: 'question', 1953: 'questioned', 1954: 'questions', 1955: 'quiksilver', 1956: 'quit', 1957: 'quits', 1958: 'quiz', 1959: 'quotes', 1960: 'qwest', 1961: 'race', 1962: 'racism', 1963: 'radcliffe', 1964: 'radio', 1965: 'raid', 1966: 'rail', 1967: 'raised', 1968: 'raises', 1969: 'rallies', 1970: 'rally', 1971: 'range', 1972: 'rangers', 1973: 'ranieri', 1974: 'rap', 1975: 'raped', 1976: 'rapid', 1977: 'rapped', 1978: 'rapper', 1979: 'raps', 1980: 'rate', 1981: 'rates', 1982: 'rattle', 1983: 'ray', 1984: 'reach', 1985: 'reaches', 1986: 'reaction', 1987: 'reading', 1988: 'ready', 1989: 'real', 1990: 'really', 1991: 'rebellion', 1992: 'reboot', 1993: 'rebound', 1994: 'rebounds', 1995: 'rebuilds', 1996: 'rebuts', 1997: 'recall', 1998: 'recalls', 1999: 'receives', 2000: 'recession', 2001: 'record', 2002: 'recordings', 2003: 'recovery', 2004: 'recreates', 2005: 'recruit', 2006: 'red', 2007: 'redknapp', 2008: 'reds', 2009: 'reeves', 2010: 'ref', 2011: 'referendum', 2012: 'reform', 2013: 'reforms', 2014: 'refuge', 2015: 'refuse', 2016: 'refuses', 2017: 'regains', 2018: 'regime', 2019: 'regiments', 2020: 'regulator', 2021: 'rein', 2022: 'reject', 2023: 'rejected', 2024: 'rejects', 2025: 'relay', 2026: 'release', 2027: 'releases', 2028: 'reliance', 2029: 'relief', 2030: 'relieved', 2031: 'relishes', 2032: 'relishing', 2033: 'rem', 2034: 'remains', 2035: 'remark', 2036: 'remember', 2037: 'remote', 2038: 'renewed', 2039: 'rented', 2040: 'replace', 2041: 'replacement', 2042: 'report', 2043: 'reports', 2044: 'reprieve', 2045: 'republic', 2046: 'rescue', 2047: 'rescued', 2048: 'rescues', 2049: 'research', 2050: 'reserves', 2051: 'resign', 2052: 'resigns', 2053: 'respect', 2054: 'respond', 2055: 'rest', 2056: 'retail', 2057: 'retailers', 2058: 'retain', 2059: 'retains', 2060: 'rethink', 2061: 'retirement', 2062: 'retract', 2063: 'return', 2064: 'returns', 2065: 'reunite', 2066: 'reuters', 2067: 'revamp', 2068: 'reveal', 2069: 'revealed', 2070: 'reveals', 2071: 'revel', 2072: 'revenge', 2073: 'reverse', 2074: 'reviews', 2075: 'revival', 2076: 'revive', 2077: 'revolution', 2078: 'revolutionise', 2079: 'reyes', 2080: 'reynolds', 2081: 'rfid', 2082: 'rich', 2083: 'ridiculed', 2084: 'rifle', 2085: 'rift', 2086: 'rights', 2087: 'rings', 2088: 'ringtone', 2089: 'riots', 2090: 'rise', 2091: 'rises', 2092: 'rising', 2093: 'risk', 2094: 'rival', 2095: 'rivals', 2096: 'road', 2097: 'robben', 2098: 'roberts', 2099: 'robertson', 2100: 'robinson', 2101: 'robot', 2102: 'robotic', 2103: 'robots', 2104: 'robson', 2105: 'rochus', 2106: 'rock', 2107: 'rockers', 2108: 'rocks', 2109: 'roddick', 2110: 'role', 2111: 'rome', 2112: 'ronaldo', 2113: 'rossignol', 2114: 'roundabout', 2115: 'rovers', 2116: 'row', 2117: 'rowlands', 2118: 'roxy', 2119: 'royal', 2120: 'royale', 2121: 'rues', 2122: 'rugby', 2123: 'rule', 2124: 'ruled', 2125: 'rules', 2126: 'ruling', 2127: 'rumour', 2128: 'run', 2129: 'runners', 2130: 'running', 2131: 'rusedski', 2132: 'rush', 2133: 'russia', 2134: 'russian', 2135: 'rwanda', 2136: 'rwandan', 2137: 's', 2138: 'sa', 2139: 'saab', 2140: 'sabbath', 2141: 'sack', 2142: 'sacked', 2143: 'safety', 2144: 'safin', 2145: 'sail', 2146: 'sailing', 2147: 'saints', 2148: 'salary', 2149: 'sale', 2150: 'sales', 2151: 'sampling', 2152: 'san', 2153: 'santini', 2154: 'santy', 2155: 'satellite', 2156: 'saudi', 2157: 'saulnier', 2158: 'savage', 2159: 'save', 2160: 'savour', 2161: 'savours', 2162: 'savoy', 2163: 'savvy', 2164: 'sayeed', 2165: 'says', 2166: 'scam', 2167: 'scams', 2168: 'scan', 2169: 'scandal', 2170: 'scares', 2171: 'scene', 2172: 'schedule', 2173: 'scheme', 2174: 'school', 2175: 'schools', 2176: 'scissor', 2177: 'scoggins', 2178: 'scoops', 2179: 'score', 2180: 'scores', 2181: 'scotland', 2182: 'scots', 2183: 'scott', 2184: 'scottish', 2185: 'scrapped', 2186: 'screen', 2187: 'screened', 2188: 'screens', 2189: 'screensaver', 2190: 'scrutiny', 2191: 'sculthorpe', 2192: 'sea', 2193: 'seal', 2194: 'seals', 2195: 'seamen', 2196: 'search', 2197: 'searchers', 2198: 'season', 2199: 'seasonal', 2200: 'seat', 2201: 'sec', 2202: 'second', 2203: 'secret', 2204: 'secures', 2205: 'security', 2206: 'see', 2207: 'seek', 2208: 'seeking', 2209: 'seeks', 2210: 'sees', 2211: 'select', 2212: 'sell', 2213: 'sella', 2214: 'sellers', 2215: 'selling', 2216: 'sells', 2217: 'senior', 2218: 'sensation', 2219: 'sentence', 2220: 'sequel', 2221: 'serena', 2222: 'series', 2223: 'servants', 2224: 'service', 2225: 'services', 2226: 'set', 2227: 'setback', 2228: 'sets', 2229: 'settles', 2230: 'sevens', 2231: 'sexism', 2232: 'sfa', 2233: 'shake', 2234: 'shape', 2235: 'shareholder', 2236: 'shares', 2237: 'sharply', 2238: 'shed', 2239: 'sheffield', 2240: 'shelves', 2241: 'shine', 2242: 'shock', 2243: 'shocks', 2244: 'shooting', 2245: 'shoppers', 2246: 'shopping', 2247: 'short', 2248: 'shortlist', 2249: 'should', 2250: 'shoulders', 2251: 'show', 2252: 'shows', 2253: 'shrugs', 2254: 'shun', 2255: 'sidelined', 2256: 'sidelines', 2257: 'sideways', 2258: 'sights', 2259: 'sign', 2260: 'signal', 2261: 'signals', 2262: 'signs', 2263: 'silent', 2264: 'simpsons', 2265: 'singer', 2266: 'single', 2267: 'singles', 2268: 'sir', 2269: 'sisters', 2270: 'sitcom', 2271: 'site', 2272: 'sites', 2273: 'six', 2274: 'sizzla', 2275: 'skates', 2276: 'skies', 2277: 'skipper', 2278: 'skulls', 2279: 'sky', 2280: 'slam', 2281: 'slater', 2282: 'slavery', 2283: 'slides', 2284: 'slight', 2285: 'slim', 2286: 'slimmer', 2287: 'slips', 2288: 'slogan', 2289: 'slopping', 2290: 'slovakia', 2291: 'slow', 2292: 'slowdown', 2293: 'sluggish', 2294: 'slum', 2295: 'slump', 2296: 'slumps', 2297: 'smart', 2298: 'smartphones', 2299: 'smash', 2300: 'smashed', 2301: 'smith', 2302: 'smoking', 2303: 'snack', 2304: 'snap', 2305: 'snicket', 2306: 'snow', 2307: 'snowball', 2308: 'soap', 2309: 'soar', 2310: 'sociedad', 2311: 'soderling', 2312: 'softbank', 2313: 'software', 2314: 'solid', 2315: 'solskjaer', 2316: 'solutions', 2317: 'song', 2318: 'sony', 2319: 'soros', 2320: 'sorry', 2321: 'sotherton', 2322: 'soul', 2323: 'sound', 2324: 'souness', 2325: 'source', 2326: 'sources', 2327: 'south', 2328: 'southampton', 2329: 'space', 2330: 'spacey', 2331: 'spain', 2332: 'spam', 2333: 'spanish', 2334: 'spark', 2335: 'sparks', 2336: 'speak', 2337: 'speaker', 2338: 'spears', 2339: 'special', 2340: 'spector', 2341: 'speech', 2342: 'speed', 2343: 'speedy', 2344: 'spending', 2345: 'spices', 2346: 'spider', 2347: 'spike', 2348: 'spirit', 2349: 'spit', 2350: 'split', 2351: 'splits', 2352: 'sport', 2353: 'sporting', 2354: 'sports', 2355: 'spot', 2356: 'spotlight', 2357: 'spotted', 2358: 'spread', 2359: 'spree', 2360: 'springer', 2361: 'sprint', 2362: 'sprinter', 2363: 'sprinters', 2364: 'spurs', 2365: 'spurt', 2366: 'spyware', 2367: 'squabbles', 2368: 'squad', 2369: 'ssl', 2370: 'stadium', 2371: 'staff', 2372: 'stage', 2373: 'stake', 2374: 'stakes', 2375: 'stalemate', 2376: 'stallone', 2377: 'stam', 2378: 'stan', 2379: 'stand', 2380: 'standard', 2381: 'standby', 2382: 'stands', 2383: 'star', 2384: 'stars', 2385: 'start', 2386: 'starts', 2387: 'state', 2388: 'station', 2389: 'stations', 2390: 'stay', 2391: 'stays', 2392: 'steady', 2393: 'steams', 2394: 'steel', 2395: 'steer', 2396: 'steering', 2397: 'stem', 2398: 'step', 2399: 'stepping', 2400: 'steps', 2401: 'stern', 2402: 'still', 2403: 'stir', 2404: 'stock', 2405: 'stockmarket', 2406: 'stocks', 2407: 'stop', 2408: 'storage', 2409: 'stormy', 2410: 'story', 2411: 'strachan', 2412: 'straw', 2413: 'streak', 2414: 'street', 2415: 'strengthened', 2416: 'stress', 2417: 'stresses', 2418: 'strike', 2419: 'striker', 2420: 'strikes', 2421: 'stroke', 2422: 'stroll', 2423: 'strong', 2424: 'struggles', 2425: 'stuart', 2426: 'student', 2427: 'students', 2428: 'studio', 2429: 'stunned', 2430: 'subsidies', 2431: 'succeeding', 2432: 'success', 2433: 'succession', 2434: 'successor', 2435: 'sue', 2436: 'sues', 2437: 'suez', 2438: 'suffer', 2439: 'suffers', 2440: 'suing', 2441: 'suitor', 2442: 'summer', 2443: 'summit', 2444: 'sun', 2445: 'sundance', 2446: 'sunderland', 2447: 'super', 2448: 'supercomputer', 2449: 'supercomputing', 2450: 'superhero', 2451: 'superstar', 2452: 'supplements', 2453: 'suppliers', 2454: 'supply', 2455: 'support', 2456: 'surfers', 2457: 'surge', 2458: 'surges', 2459: 'surrounds', 2460: 'survey', 2461: 'survival', 2462: 'survives', 2463: 'suspects', 2464: 'suspend', 2465: 'suspended', 2466: 'suspension', 2467: 'suspensions', 2468: 'swallows', 2469: 'swank', 2470: 'swap', 2471: 'swearing', 2472: 'sweden', 2473: 'sweep', 2474: 'swing', 2475: 'swipe', 2476: 'swiss', 2477: 'switch', 2478: 'switched', 2479: 'sydney', 2480: 'system', 2481: 't', 2482: 'tackle', 2483: 'tackles', 2484: 'tags', 2485: 'tait', 2486: 'take', 2487: 'takeover', 2488: 'takes', 2489: 'takings', 2490: 'tale', 2491: 'talent', 2492: 'talk', 2493: 'talks', 2494: 'tannadice', 2495: 'tape', 2496: 'tarantino', 2497: 'target', 2498: 'targets', 2499: 'tariff', 2500: 'task', 2501: 'tasks', 2502: 'taunts', 2503: 'tautou', 2504: 'tax', 2505: 'taylor', 2506: 'team', 2507: 'tears', 2508: 'tech', 2509: 'technology', 2510: 'teenager', 2511: 'teenagers', 2512: 'telecom', 2513: 'telecoms', 2514: 'telekom', 2515: 'television', 2516: 'telewest', 2517: 'tells', 2518: 'temple', 2519: 'tennis', 2520: 'tense', 2521: 'term', 2522: 'terrestrial', 2523: 'terror', 2524: 'test', 2525: 'tetris', 2526: 'text', 2527: 'textile', 2528: 'thai', 2529: 'than', 2530: 'thanou', 2531: 'the', 2532: 'theatre', 2533: 'theft', 2534: 'their', 2535: 'third', 2536: 'thomas', 2537: 'thompson', 2538: 'thousands', 2539: 'threat', 2540: 'threaten', 2541: 'threatens', 2542: 'three', 2543: 'thrilled', 2544: 'through', 2545: 'tickets', 2546: 'tie', 2547: 'tight', 2548: 'tills', 2549: 'time', 2550: 'timed', 2551: 'tindall', 2552: 'tinkers', 2553: 'tipped', 2554: 'tips', 2555: 'title', 2556: 'to', 2557: 'tobacco', 2558: 'told', 2559: 'toll', 2560: 'tom', 2561: 'tomlinson', 2562: 'too', 2563: 'tools', 2564: 'top', 2565: 'tops', 2566: 'tories', 2567: 'tory', 2568: 'total', 2569: 'totp', 2570: 'touch', 2571: 'tough', 2572: 'tour', 2573: 'tout', 2574: 'toxic', 2575: 'tracking', 2576: 'tracks', 2577: 'trade', 2578: 'trader', 2579: 'trail', 2580: 'trails', 2581: 'train', 2582: 'training', 2583: 'transport', 2584: 'travel', 2585: 'travels', 2586: 'treatment', 2587: 'trek', 2588: 'trial', 2589: 'tribute', 2590: 'tricked', 2591: 'triggers', 2592: 'trims', 2593: 'trio', 2594: 'trip', 2595: 'triple', 2596: 'triples', 2597: 'triumph', 2598: 'trojan', 2599: 'troops', 2600: 'trophy', 2601: 'trouble', 2602: 'troubled', 2603: 'truce', 2604: 'trust', 2605: 'try', 2606: 'tsunami', 2607: 'tube', 2608: 'tulu', 2609: 'tune', 2610: 'tunnel', 2611: 'turbo', 2612: 'turkey', 2613: 'turkish', 2614: 'turkmen', 2615: 'turn', 2616: 'turnaround', 2617: 'turnout', 2618: 'turns', 2619: 'tv', 2620: 'twins', 2621: 'two', 2622: 'uefa', 2623: 'uk', 2624: 'ukip', 2625: 'ukraine', 2626: 'ultimatum', 2627: 'umaga', 2628: 'uncapped', 2629: 'uncertain', 2630: 'uncertainty', 2631: 'unclear', 2632: 'under', 2633: 'underpin', 2634: 'unexpected', 2635: 'unfazed', 2636: 'unhappy', 2637: 'unilever', 2638: 'unit', 2639: 'united', 2640: 'unites', 2641: 'unity', 2642: 'unveil', 2643: 'unveiled', 2644: 'unveils', 2645: 'unwelcome', 2646: 'up', 2647: 'upbeat', 2648: 'urge', 2649: 'urged', 2650: 'urges', 2651: 'us', 2652: 'user', 2653: 'users', 2654: 'usher', 2655: 'utd', 2656: 'v', 2657: 'valencia', 2658: 'van', 2659: 'varig', 2660: 'venezuela', 2661: 'venezuelan', 2662: 'venue', 2663: 'venus', 2664: 'vera', 2665: 'verdict', 2666: 'veritas', 2667: 'versace', 2668: 'versus', 2669: 'veteran', 2670: 'vibe', 2671: 'vickery', 2672: 'victims', 2673: 'victory', 2674: 'video', 2675: 'vie', 2676: 'vies', 2677: 'viewers', 2678: 'viewing', 2679: 'vinci', 2680: 'violence', 2681: 'viotti', 2682: 'virus', 2683: 'viruses', 2684: 'visa', 2685: 'vision', 2686: 'visit', 2687: 'visits', 2688: 'visual', 2689: 'vivendi', 2690: 'vocal', 2691: 'vodafone', 2692: 'volcano', 2693: 'vote', 2694: 'voters', 2695: 'vows', 2696: 'vw', 2697: 'wada', 2698: 'wades', 2699: 'waiting', 2700: 'waits', 2701: 'wales', 2702: 'walker', 2703: 'wall', 2704: 'want', 2705: 'wants', 2706: 'war', 2707: 'wares', 2708: 'warm', 2709: 'warner', 2710: 'warning', 2711: 'warnings', 2712: 'warns', 2713: 'wars', 2714: 'wasps', 2715: 'watch', 2716: 'watchdog', 2717: 'watching', 2718: 'water', 2719: 'wave', 2720: 'waves', 2721: 'way', 2722: 'weak', 2723: 'weather', 2724: 'web', 2725: 'website', 2726: 'websites', 2727: 'wed', 2728: 'weeks', 2729: 'weir', 2730: 'welcome', 2731: 'welsh', 2732: 'wenger', 2733: 'what', 2734: 'wheel', 2735: 'when', 2736: 'while', 2737: 'whingeing', 2738: 'whitbread', 2739: 'white', 2740: 'why', 2741: 'widen', 2742: 'widens', 2743: 'wight', 2744: 'wilkinson', 2745: 'will', 2746: 'williams', 2747: 'wilson', 2748: 'wimbledon', 2749: 'win', 2750: 'windows', 2751: 'winds', 2752: 'wine', 2753: 'winemaker', 2754: 'winners', 2755: 'winning', 2756: 'wins', 2757: 'wintry', 2758: 'wipro', 2759: 'wireless', 2760: 'with', 2761: 'withdrawn', 2762: 'witness', 2763: 'wmc', 2764: 'wobble', 2765: 'wolves', 2766: 'woman', 2767: 'women', 2768: 'wonder', 2769: 'woo', 2770: 'woodward', 2771: 'woolf', 2772: 'word', 2773: 'work', 2774: 'workers', 2775: 'world', 2776: 'worldcom', 2777: 'worlds', 2778: 'worm', 2779: 'worries', 2780: 'wow', 2781: 'writer', 2782: 'writing', 2783: 'wrong', 2784: 'wru', 2785: 'wto', 2786: 'x', 2787: 'xp', 2788: 'xstrata', 2789: 'yachvili', 2790: 'yahoo', 2791: 'yeading', 2792: 'year', 2793: 'years', 2794: 'yet', 2795: 'york', 2796: 'you', 2797: 'young', 2798: 'youngsters', 2799: 'your', 2800: 'yourself', 2801: 'yukos', 2802: 'zambia', 2803: 'zeros', 2804: 'zzzzz'}\n"
     ]
    }
   ],
   "source": [
    "word_to_ix = { ch:i for i,ch in enumerate(sorted(words)) }\n",
    "ix_to_word = { i:ch for i,ch in enumerate(sorted(words)) }\n",
    "print(ix_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "\n",
    "def smooth(loss, cur_loss):\n",
    "    return loss * 0.999 + cur_loss * 0.001\n",
    "\n",
    "def print_sample(sample_ix, ix_to_char):\n",
    "    txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "    txt = txt[0].upper() + txt[1:]  # capitalize first character \n",
    "    print ('%s' % (txt, ), end='')\n",
    "\n",
    "def get_initial_loss(vocab_size, seq_length):\n",
    "    return -np.log(1.0/vocab_size)*seq_length\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "\n",
    "def initialize_parameters(n_a, n_x, n_y):\n",
    "    \"\"\"\n",
    "    Initialize parameters with small random values\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing:\n",
    "                        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)\n",
    "                        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)\n",
    "                        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n",
    "                        b --  Bias, numpy array of shape (n_a, 1)\n",
    "                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n",
    "    \"\"\"\n",
    "    np.random.seed(1)\n",
    "    Wax = np.random.randn(n_a, n_x)*0.01 # input to hidden\n",
    "    Waa = np.random.randn(n_a, n_a)*0.01 # hidden to hidden\n",
    "    Wya = np.random.randn(n_y, n_a)*0.01 # hidden to output\n",
    "    b = np.zeros((n_a, 1)) # hidden bias\n",
    "    by = np.zeros((n_y, 1)) # output bias\n",
    "    \n",
    "    parameters = {\"Wax\": Wax, \"Waa\": Waa, \"Wya\": Wya, \"b\": b,\"by\": by}\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "def rnn_step_forward(parameters, a_prev, x):\n",
    "    \n",
    "    Waa, Wax, Wya, by, b = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['b']\n",
    "    a_next = np.tanh(np.dot(Wax, x) + np.dot(Waa, a_prev) + b) # hidden state\n",
    "    p_t = softmax(np.dot(Wya, a_next) + by) # unnormalized log probabilities for next chars # probabilities for next chars \n",
    "    \n",
    "    return a_next, p_t\n",
    "\n",
    "def rnn_step_backward(dy, gradients, parameters, x, a, a_prev):\n",
    "    \n",
    "    gradients['dWya'] += np.dot(dy, a.T)\n",
    "    gradients['dby'] += dy\n",
    "    da = np.dot(parameters['Wya'].T, dy) + gradients['da_next'] # backprop into h\n",
    "    daraw = (1 - a * a) * da # backprop through tanh nonlinearity\n",
    "    gradients['db'] += daraw\n",
    "    gradients['dWax'] += np.dot(daraw, x.T)\n",
    "    gradients['dWaa'] += np.dot(daraw, a_prev.T)\n",
    "    gradients['da_next'] = np.dot(parameters['Waa'].T, daraw)\n",
    "    return gradients\n",
    "\n",
    "def update_parameters(parameters, gradients, lr):\n",
    "\n",
    "    parameters['Wax'] += -lr * gradients['dWax']\n",
    "    parameters['Waa'] += -lr * gradients['dWaa']\n",
    "    parameters['Wya'] += -lr * gradients['dWya']\n",
    "    parameters['b']  += -lr * gradients['db']\n",
    "    parameters['by']  += -lr * gradients['dby']\n",
    "    return parameters\n",
    "\n",
    "def rnn_forward(X, Y, a0, parameters, vocab_size = 27):\n",
    "    \n",
    "    # Initialize x, a and y_hat as empty dictionaries\n",
    "    x, a, y_hat = {}, {}, {}\n",
    "    \n",
    "    a[-1] = np.copy(a0)\n",
    "    \n",
    "    # initialize your loss to 0\n",
    "    loss = 0\n",
    "    \n",
    "    for t in range(len(X)):\n",
    "        \n",
    "        # Set x[t] to be the one-hot vector representation of the t'th character in X.\n",
    "        # if X[t] == None, we just have x[t]=0. This is used to set the input for the first timestep to the zero vector. \n",
    "        x[t] = np.zeros((vocab_size,1)) \n",
    "        if (X[t] != None):\n",
    "            x[t][X[t]] = 1\n",
    "        \n",
    "        # Run one step forward of the RNN\n",
    "        a[t], y_hat[t] = rnn_step_forward(parameters, a[t-1], x[t])\n",
    "        \n",
    "        # Update the loss by substracting the cross-entropy term of this time-step from it.\n",
    "        loss -= np.log(y_hat[t][Y[t],0])\n",
    "        \n",
    "    cache = (y_hat, a, x)\n",
    "        \n",
    "    return loss, cache\n",
    "\n",
    "def rnn_backward(X, Y, parameters, cache):\n",
    "    # Initialize gradients as an empty dictionary\n",
    "    gradients = {}\n",
    "    \n",
    "    # Retrieve from cache and parameters\n",
    "    (y_hat, a, x) = cache\n",
    "    Waa, Wax, Wya, by, b = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['b']\n",
    "    \n",
    "    # each one should be initialized to zeros of the same dimension as its corresponding parameter\n",
    "    gradients['dWax'], gradients['dWaa'], gradients['dWya'] = np.zeros_like(Wax), np.zeros_like(Waa), np.zeros_like(Wya)\n",
    "    gradients['db'], gradients['dby'] = np.zeros_like(b), np.zeros_like(by)\n",
    "    gradients['da_next'] = np.zeros_like(a[0])\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Backpropagate through time\n",
    "    for t in reversed(range(len(X))):\n",
    "        dy = np.copy(y_hat[t])\n",
    "        dy[Y[t]] -= 1\n",
    "        gradients = rnn_step_backward(dy, gradients, parameters, x[t], a[t], a[t-1])\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return gradients, a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clip(gradients, maxValue):\n",
    "    '''\n",
    "    Clips the gradients' values between minimum and maximum.\n",
    "    \n",
    "    Arguments:\n",
    "    gradients -- a dictionary containing the gradients \"dWaa\", \"dWax\", \"dWya\", \"db\", \"dby\"\n",
    "    maxValue -- everything above this number is set to this number, and everything less than -maxValue is set to -maxValue\n",
    "    \n",
    "    Returns: \n",
    "    gradients -- a dictionary with the clipped gradients.\n",
    "    '''\n",
    "    \n",
    "    dWaa, dWax, dWya, db, dby = gradients['dWaa'], gradients['dWax'], gradients['dWya'], gradients['db'], gradients['dby']\n",
    "   \n",
    "    ### START CODE HERE ###\n",
    "    # clip to mitigate exploding gradients, loop over [dWax, dWaa, dWya, db, dby]. (2 lines)\n",
    "    for gradient in [dWax, dWaa, dWya, db, dby]:\n",
    "        dWax=np.clip(dWax,-maxValue,maxValue)\n",
    "        dWaa=np.clip(dWaa,-maxValue,maxValue)\n",
    "        dWya=np.clip(dWya,-maxValue,maxValue)\n",
    "        db=np.clip(db,-maxValue,maxValue)\n",
    "        dby=np.clip(dby,-maxValue,maxValue)\n",
    "        \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    gradients = {\"dWaa\": dWaa, \"dWax\": dWax, \"dWya\": dWya, \"db\": db, \"dby\": dby}\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(X, Y, a_prev, parameters, learning_rate = 0.01):\n",
    "    \"\"\"\n",
    "    Execute one step of the optimization to train the model.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- list of integers, where each integer is a number that maps to a character in the vocabulary.\n",
    "    Y -- list of integers, exactly the same as X but shifted one index to the left.\n",
    "    a_prev -- previous hidden state.\n",
    "    parameters -- python dictionary containing:\n",
    "                        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)\n",
    "                        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)\n",
    "                        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n",
    "                        b --  Bias, numpy array of shape (n_a, 1)\n",
    "                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n",
    "    learning_rate -- learning rate for the model.\n",
    "    \n",
    "    Returns:\n",
    "    loss -- value of the loss function (cross-entropy)\n",
    "    gradients -- python dictionary containing:\n",
    "                        dWax -- Gradients of input-to-hidden weights, of shape (n_a, n_x)\n",
    "                        dWaa -- Gradients of hidden-to-hidden weights, of shape (n_a, n_a)\n",
    "                        dWya -- Gradients of hidden-to-output weights, of shape (n_y, n_a)\n",
    "                        db -- Gradients of bias vector, of shape (n_a, 1)\n",
    "                        dby -- Gradients of output bias vector, of shape (n_y, 1)\n",
    "    a[len(X)-1] -- the last hidden state, of shape (n_a, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # Forward propagate through time (1 line)\n",
    "    loss, cache = rnn_forward(X,Y,a_prev,parameters)\n",
    "    \n",
    "    # Backpropagate through time (1 line)\n",
    "    gradients, a = rnn_backward(X,Y,parameters,cache)\n",
    "    \n",
    "    # Clip your gradients between -5 (min) and 5 (max) (1 line)\n",
    "    gradients = clip(gradients, 5)\n",
    "    \n",
    "    # Update parameters (1 line)\n",
    "    parameters = update_parameters(parameters, gradients, learning_rate)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return loss, gradients, a[len(X)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: sample\n",
    "\n",
    "def sample(parameters, char_to_ix):\n",
    "    \"\"\"\n",
    "    Sample a sequence of characters according to a sequence of probability distributions output of the RNN\n",
    "\n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing the parameters Waa, Wax, Wya, by, and b. \n",
    "    char_to_ix -- python dictionary mapping each character to an index.\n",
    "    seed -- used for grading purposes. Do not worry about it.\n",
    "\n",
    "    Returns:\n",
    "    indices -- a list of length n containing the indices of the sampled characters.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve parameters and relevant shapes from \"parameters\" dictionary\n",
    "    Waa, Wax, Wya, by, b = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['b']\n",
    "    vocab_size = by.shape[0]\n",
    "    n_a = Waa.shape[1]\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Step 1: Create the one-hot vector x for the first character (initializing the sequence generation). (1 line)\n",
    "    x = np.zeros((len(words),1))\n",
    "    # Step 1': Initialize a_prev as zeros (1 line)\n",
    "    a_prev = np.zeros((n_a,1))\n",
    "    \n",
    "    \n",
    "    # Create an empty list of indices, this is the list which will contain the list of indices of the characters to generate (1 line)\n",
    "    indices = []\n",
    "    \n",
    "    # Idx is a flag to detect a newline character, we initialize it to -1\n",
    "    idx = -1 \n",
    "    \n",
    "    # Loop over time-steps t. At each time-step, sample a character from a probability distribution and append \n",
    "    # its index to \"indices\". We'll stop if we reach 50 characters (which should be very unlikely with a well \n",
    "    # trained model), which helps debugging and prevents entering an infinite loop. \n",
    "    newline_character = char_to_ix['zzzzz']\n",
    "    \n",
    "    \n",
    "    while (idx != newline_character and counter != 50):\n",
    "        \n",
    "        # Step 2: Forward propagate x using the equations (1), (2) and (3)\n",
    "        a = np.tanh(np.dot(Wax,x)+np.dot(Waa,a_prev)+b)\n",
    "        z = np.dot(Wya,a)+by\n",
    "        y = softmax(z)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Step 3: Sample the index of a character within the vocabulary from the probability distribution y\n",
    "        idx = np.random.choice(2805,p=y.ravel())\n",
    "\n",
    "        \n",
    "        # Append the index to \"indices\"\n",
    "        indices.append(idx)\n",
    "        \n",
    "        # Step 4: Overwrite the input character as the one corresponding to the sampled index.\n",
    "        x = np.zeros((len(words),1))\n",
    "        x[idx] = 1\n",
    "        \n",
    "        # Update \"a_prev\" to be \"a\"\n",
    "        a_prev = a\n",
    "        \n",
    "        \n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    if (counter == 50):\n",
    "        indices.append(word_to_ix['zzzzz'])\n",
    "    \n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-40-88982d4b3edd>, line 30)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-40-88982d4b3edd>\"\u001b[0;36m, line \u001b[0;32m30\u001b[0m\n\u001b[0;31m    examples[]\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# GRADED FUNCTION: model\n",
    "\n",
    "def model(data, ix_to_word, word_to_ix, num_iterations = 35000, n_a = 50, dino_names = 7, vocab_size = 2805):\n",
    "    \"\"\"\n",
    "    Trains the model and generates dinosaur names. \n",
    "    \n",
    "    Arguments:\n",
    "    data -- text corpus\n",
    "    ix_to_char -- dictionary that maps the index to a character\n",
    "    char_to_ix -- dictionary that maps a character to an index\n",
    "    num_iterations -- number of iterations to train the model for\n",
    "    n_a -- number of units of the RNN cell\n",
    "    dino_names -- number of dinosaur names you want to sample at each iteration. \n",
    "    vocab_size -- number of unique characters found in the text, size of the vocabulary\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- learned parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve n_x and n_y from vocab_size\n",
    "    n_x, n_y = vocab_size, vocab_size\n",
    "    \n",
    "    # Initialize parameters\n",
    "    parameters = initialize_parameters(n_a, n_x, n_y)\n",
    "    \n",
    "    # Initialize loss (this is required because we want to smooth our loss, don't worry about it)\n",
    "    loss = get_initial_loss(vocab_size, dino_names)\n",
    "    \n",
    "    # Build list of all dinosaur names (training examples).\n",
    "    examples[]\n",
    "    with open(\"data.txt\") as f:\n",
    "        line = f.readlines()\n",
    "        line = line.replace(\"\\n\", \" zzzzz\").strip() \n",
    "        words=[]\n",
    "        for word in line.split():\n",
    "           words.append(word)\n",
    "        examples.append[words]\n",
    "        \n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "    # Shuffle list of all dinosaur names\n",
    "    np.random.shuffle(examples)\n",
    "    \n",
    "    # Initialize the hidden state of your LSTM\n",
    "    a_prev = np.zeros((n_a, 1))\n",
    "    \n",
    "    # Optimization loop\n",
    "    for j in range(num_iterations):\n",
    "        \n",
    "        ### START CODE HERE ###\n",
    "        \n",
    "        # Use the hint above to define one training example (X,Y) ( 2 lines)\n",
    "        index = j % len(examples)\n",
    "        X = [None] + [word_to_ix[ch] for ch in examples[index]] \n",
    "        Y =  X[1:] + [word_to_ix[\"zzzzz\"]]\n",
    "        \n",
    "        # Perform one optimization step: Forward-prop -> Backward-prop -> Clip -> Update parameters\n",
    "        # Choose a learning rate of 0.01\n",
    "        curr_loss, gradients, a_prev = optimize(X, Y, a_prev, parameters,0.01)\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Use a latency trick to keep the loss smooth. It happens here to accelerate the training.\n",
    "        loss = smooth(loss, curr_loss)\n",
    "\n",
    "        # Every 2000 Iteration, generate \"n\" characters thanks to sample() to check if the model is learning properly\n",
    "        if j % 2000 == 0:\n",
    "            \n",
    "            print('Iteration: %d, Loss: %f' % (j, loss) + '\\n')\n",
    "            \n",
    "            # The number of dinosaur names to print\n",
    "            seed = 0\n",
    "            for name in range(dino_names):\n",
    "                \n",
    "                # Sample indices and print them\n",
    "                sampled_indices = sample(parameters, char_to_ix, seed)\n",
    "                print_sample(sampled_indices, ix_to_char)\n",
    "                \n",
    "                seed += 1  # To get the same result for grading purposed, increment the seed by one. \n",
    "      \n",
    "            print('\\n')\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'i'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-2935fb057735>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mix_to_word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_to_ix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-38-c246a62b16f2>\u001b[0m in \u001b[0;36mmodel\u001b[0;34m(data, ix_to_word, word_to_ix, num_iterations, n_a, dino_names, vocab_size)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;31m# Use the hint above to define one training example (X,Y) ( 2 lines)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword_to_ix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mch\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword_to_ix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"zzzzz\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-38-c246a62b16f2>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;31m# Use the hint above to define one training example (X,Y) ( 2 lines)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword_to_ix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mch\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword_to_ix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"zzzzz\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'i'"
     ]
    }
   ],
   "source": [
    "parameters = model(data, ix_to_word, word_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
